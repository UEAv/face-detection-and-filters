{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "341d8d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135e94e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These constants define different image filters or modes: Preview, Blur, Features, \n",
    "# and Canny Edge Detection.\n",
    "PREVIEW = 0\n",
    "BLUR = 1\n",
    "FEATURES = 2\n",
    "CANNY = 3\n",
    "\n",
    "# This dictionary feature_params contains parameters used for feature detection, such as maximum corners,\n",
    "#  quality level, minimum distance, and block size.\n",
    "feature_params = dict(maxCorners = 500,\n",
    "                     qualityLevel = 0.2,\n",
    "                     minDistance = 15,\n",
    "                     blockSize = 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cadc572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNetFromCaffe('deploy.prototxt','res10_300x300_ssd_iter_140000_fp16.caffemodel')\n",
    "\n",
    "\n",
    "# These variables define parameters such as input width and height of the model, mean values \n",
    "# for normalization, and confidence threshold for detections.\n",
    "# make sure the parameter is the same as the one that was use to train the model\n",
    "in_width = 300\n",
    "in_height = 300\n",
    "mean = [104, 117, 123]\n",
    "conf_threshold = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ca42bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines initialize variables s (video source), image_filter (current filter mode), \n",
    "# alive (flag to control the loop), and result (variable to hold processed frames).\n",
    "s = 0\n",
    "image_filter = PREVIEW\n",
    "alive = True\n",
    "result = None\n",
    "\n",
    "# This code sets up a window with the name 'Camera Filters' using namedWindow() function from OpenCV.\n",
    "win_name = 'Camera Filters'\n",
    "cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n",
    "\n",
    "# This line creates a video capture object source using the video source specified by the variable s.\n",
    "source = cv2.VideoCapture(s)\n",
    "\n",
    "# Main loop for processing frames:\n",
    "while alive:\n",
    "    has_frame, frame = source.read()\n",
    "    if not has_frame:\n",
    "        break\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "      #  Here, each frame is read from the video source, flipped horizontally, and preprocessed as\n",
    "    #  required by the deep learning model. Then, inference is performed on the preprocessed frame.\n",
    "    has_frame, frame = source.read()\n",
    "    frame = cv2.flip(frame, 1) #just to flip the frame to remove literal invertion for this tutorial\n",
    "    frame_height = frame.shape[0]\n",
    "    frame_width = frame.shape[1]\n",
    "    #create a 4D blob from a frame\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (in_width, in_height), mean, swapRB = False, crop = False)\n",
    "    #run the model\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    \n",
    "    # This loop iterates over the detections, filters out those with confidence above the threshold, \n",
    "    # and draws bounding boxes around the detected objects on the frame.\n",
    "    for i in range (detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            x_left_bottom = int(detections[0, 0, i, 3] * frame_width)\n",
    "            y_left_bottom = int(detections[0, 0, i, 4] * frame_height)\n",
    "            y_right_top = int(detections[0, 0, i, 5] * frame_width)\n",
    "            x_right_top = int(detections[0, 0, i, 6] * frame_height)\n",
    "            \n",
    "            cv2.rectangle(frame, (x_left_bottom, y_left_bottom), (x_right_top, y_right_top), (0, 255, 0))\n",
    "            label = \"Confidence: %.4f\" % confidence\n",
    "            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "            \n",
    "            cv2.rectangle(frame, (x_left_bottom, y_left_bottom-label_size[1]),\n",
    "                                ( x_left_bottom + label_size[0], y_left_bottom + base_line),\n",
    "                         (255, 255, 255), cv2.FILLED)\n",
    "            cv2.putText(frame, label, (x_left_bottom, y_left_bottom),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n",
    "    \n",
    "    # Here, the inference time is calculated and displayed on the frame.\n",
    "    t,_ = net.getPerfProfile()\n",
    "    label = \"Inference time: %.2f ms\" % (t * 1000.0/cv2.getTickFrequency())\n",
    "    cv2.putText(frame, label, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    frame = cv2.flip(frame,1)\n",
    "    \n",
    "    # Apply image filters based on user input:\n",
    "    if image_filter == PREVIEW:\n",
    "        result = frame\n",
    "    elif image_filter == CANNY:\n",
    "        result = cv2.Canny(frame, 145, 150)\n",
    "    elif image_filter == BLUR:\n",
    "        result = cv2.blur(frame, (13,13))\n",
    "    elif image_filter == FEATURES:\n",
    "        result = frame\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        corners = cv2.goodFeaturesToTrack(frame_gray, **feature_params)\n",
    "        if corners is not None:\n",
    "            for x, y in numpy.float32(corners).reshape(-1,2):\n",
    "                cv2.circle(result, (int(x),int(y)), 10, (0, 255, 0), 1)\n",
    "                \n",
    "    # This block displays the processed frame in the window and waits for user input.\n",
    "    #  Based on the key pressed by the user, it changes the image_filter mode or exits the loop if\n",
    "    #  'Q' or 'q' or Escape key (ASCII 27) is pressed.\n",
    "    cv2.imshow(win_name, result)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('Q') or key == ord('q') or key == 27:\n",
    "        alive = False\n",
    "    elif key == ord('C') or key == ord('c'):\n",
    "        image_filter = CANNY\n",
    "    elif key == ord('B') or key == ord('b'):\n",
    "        image_filter = BLUR\n",
    "    elif key == ord('F') or key == ord('f'):\n",
    "        image_filter = FEATURES\n",
    "    elif key == ord('P') or key == ord('p'):\n",
    "        image_filter = PREVIEW\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "        \n",
    "# Finally, it releases the video source and destroys the window when the loop exits.        \n",
    "source.release()\n",
    "cv2.destroyWindow(win_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e7961d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e8e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
